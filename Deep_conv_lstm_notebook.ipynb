{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00226/OpportunityUCIDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: preprocess_data.py [-h] -i INPUT -o OUTPUT [-t {gestures,locomotion}]\n",
      "\n",
      "Preprocess OPPORTUNITY dataset\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -i INPUT, --input INPUT\n",
      "                        OPPORTUNITY zip file\n",
      "  -o OUTPUT, --output OUTPUT\n",
      "                        Processed data file\n",
      "  -t {gestures,locomotion}, --task {gestures,locomotion}\n",
      "                        Type of activities to be recognized\n"
     ]
    }
   ],
   "source": [
    "!python3 preprocess_data.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset data/OpportunityUCIDataset.zip\n",
      "Processing dataset files ...\n",
      "... file OpportunityUCIDataset/dataset/S1-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S1-ADL5.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-Drill.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL1.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL2.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL3.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S2-ADL5.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL4.dat\n",
      "... file OpportunityUCIDataset/dataset/S3-ADL5.dat\n",
      "Final datasets with size: | train (557963, 113) | test (118750, 113) | \n"
     ]
    }
   ],
   "source": [
    "!python3 preprocess_data.py -i data/OpportunityUCIDataset.zip -o oppChallenge_gestures.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2+cu117 --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle as cp\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "#from sliding_window import sliding_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated sliding window function\n",
    "def sliding_window(a, ws, ss=None, flatten=True):\n",
    "    if ss is None:\n",
    "        ss = ws\n",
    "    ws = norm_shape(ws)\n",
    "    ss = norm_shape(ss)\n",
    "\n",
    "    ws = np.array(ws)\n",
    "    ss = np.array(ss)\n",
    "    shape = np.array(a.shape)\n",
    "\n",
    "    ls = [len(shape), len(ws), len(ss)]\n",
    "    if 1 != len(set(ls)):\n",
    "        raise ValueError('a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n",
    "\n",
    "    if np.any(ws > shape):\n",
    "        raise ValueError('ws cannot be larger than a in any dimension. a.shape was %s and ws was %s' % (str(a.shape), str(ws)))\n",
    "\n",
    "    newshape = norm_shape(((shape - ws) // ss) + 1)\n",
    "    newshape += norm_shape(ws)\n",
    "    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n",
    "    strided = np.lib.stride_tricks.as_strided(a, shape=newshape, strides=newstrides)\n",
    "    if not flatten:\n",
    "        return strided\n",
    "\n",
    "    meat = len(ws) if ws.shape else 0\n",
    "    firstdim = (np.prod(newshape[:-meat]),) if ws.shape else ()\n",
    "    dim = firstdim + newshape[-meat:]\n",
    "    dim = tuple(filter(lambda i: i != 1, dim))\n",
    "    return strided.reshape(dim)\n",
    "\n",
    "def norm_shape(shape):\n",
    "    try:\n",
    "        i = int(shape)\n",
    "        return (i,)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        t = tuple(shape)\n",
    "        return t\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    raise TypeError('shape must be an int, or a tuple of ints')\n",
    "\n",
    "# Hardcoded parameters\n",
    "NB_SENSOR_CHANNELS = 113\n",
    "NUM_CLASSES = 18\n",
    "SLIDING_WINDOW_LENGTH = 24\n",
    "SLIDING_WINDOW_STEP = 12\n",
    "BATCH_SIZE = 16\n",
    "NUM_FILTERS = 64\n",
    "FILTER_SIZE = 5\n",
    "NUM_UNITS_LSTM = 128\n",
    "LEARNING_RATE = 0.0001  # Reduced learning rate\n",
    "NUM_EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = cp.load(f)\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: train {}, test {}\".format(X_train.shape, X_test.shape))\n",
    "    return X_train.astype(np.float32), y_train.astype(np.uint8), X_test.astype(np.float32), y_test.astype(np.uint8)\n",
    "\n",
    "# Apply sliding window\n",
    "def opp_sliding_window(data_x, data_y, ws, ss):\n",
    "    data_x = sliding_window(data_x, (ws, data_x.shape[1]), (ss, 1))\n",
    "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y, ws, ss)])\n",
    "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
    "\n",
    "# Normalize data\n",
    "def normalize(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "class DeepConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepConvLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv2 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv3 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv4 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.lstm1 = nn.LSTM(NUM_FILTERS * 113, NUM_UNITS_LSTM, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(NUM_UNITS_LSTM, NUM_UNITS_LSTM, batch_first=True)\n",
    "        self.fc = nn.Linear(NUM_UNITS_LSTM, NUM_CLASSES)\n",
    "\n",
    "        # Weight initialization\n",
    "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv3.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv4.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(x.size(0), x.size(2), -1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      " ..from file data/oppChallenge_gestures.data\n",
      " ..reading instances: train (557963, 113), test (118750, 113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115940/2801352964.py:34: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  i = int(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ..after sliding window (training): inputs (46495, 24, 113), targets (46495,)\n",
      " ..after sliding window (testing): inputs (9894, 24, 113), targets (9894,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_test, y_test = load_dataset('data/oppChallenge_gestures.data')\n",
    "\n",
    "# Normalize data\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "# Apply sliding window\n",
    "X_train, y_train = opp_sliding_window(X_train, y_train, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "X_test, y_test = opp_sliding_window(X_test, y_test, SLIDING_WINDOW_LENGTH, SLIDING_WINDOW_STEP)\n",
    "print(\" ..after sliding window (training): inputs {}, targets {}\".format(X_train.shape, y_train.shape))\n",
    "print(\" ..after sliding window (testing): inputs {}, targets {}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "# Reshape data\n",
    "X_train = X_train.reshape((-1, 1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS))\n",
    "X_test = X_test.reshape((-1, 1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS))\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_data = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "model = DeepConvLSTM().to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights/DeepConvLSTM_trained_oppChallenge_gestures.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepConvLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv2 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv3 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv4 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.lstm1 = nn.LSTM(NUM_FILTERS * 113, NUM_UNITS_LSTM, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(NUM_UNITS_LSTM, NUM_UNITS_LSTM, batch_first=True)\n",
    "        self.fc = nn.Linear(NUM_UNITS_LSTM, NUM_CLASSES)\n",
    "\n",
    "        # Weight initialization\n",
    "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv3.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv4.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(x.size(0), x.size(2), -1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "model = DeepConvLSTM()\n",
    "model.load_state_dict(torch.load('weights/DeepConvLSTM_trained_oppChallenge_gestures.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepConvLSTM(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(5, 1), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(5, 1), stride=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(5, 1), stride=(1, 1))\n",
       "  (lstm1): LSTM(7232, 128, batch_first=True)\n",
       "  (lstm2): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Test Results:\n",
      "Test Accuracy: 0.8792\n",
      "Macro Precision: 0.6317\n",
      "Macro Recall: 0.5423\n",
      "Macro F1-score: 0.5617\n",
      "Average Batch Inference Time: 0.0049 seconds\n",
      "Average Per-Sample Inference Time: 0.00030426 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "test_pred = []\n",
    "test_true = []\n",
    "batch_times = []\n",
    "sample_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to('cpu'), targets.to('cpu')\n",
    "\n",
    "        # Start time for the batch\n",
    "        start_time = time.time()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # End time for the batch\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Record the inference time for the batch\n",
    "        batch_inference_time = end_time - start_time\n",
    "        batch_times.append(batch_inference_time)\n",
    "\n",
    "        # Calculate per-sample time for the batch\n",
    "        batch_size = inputs.size(0)\n",
    "        per_sample_time = batch_inference_time / batch_size\n",
    "        sample_times.append(per_sample_time)\n",
    "\n",
    "        test_pred.extend(preds.cpu().numpy())\n",
    "        test_true.extend(targets.cpu().numpy())\n",
    "\n",
    "        # Print per-batch time and per-sample time for the current batch\n",
    "        #print(f\"Batch {batch_idx + 1}:\")\n",
    "        #print(f\"  Batch Inference Time: {batch_inference_time:.4f} seconds\")\n",
    "        #print(f\"  Per-Sample Inference Time: {per_sample_time:.8f} seconds\")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_true, test_pred)\n",
    "macro_precision = precision_score(test_true, test_pred, average='macro')\n",
    "macro_recall = recall_score(test_true, test_pred, average='macro')\n",
    "macro_f1 = f1_score(test_true, test_pred, average='macro')\n",
    "\n",
    "# Calculate average batch time and average per-sample time\n",
    "average_batch_time = sum(batch_times) / len(batch_times)\n",
    "average_per_sample_time = sum(sample_times) / len(sample_times)\n",
    "\n",
    "# Results presentation\n",
    "print(f\"\\nOverall Test Results:\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "print(f\"Average Batch Inference Time: {average_batch_time:.4f} seconds\")\n",
    "print(f\"Average Per-Sample Inference Time: {average_per_sample_time:.8f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

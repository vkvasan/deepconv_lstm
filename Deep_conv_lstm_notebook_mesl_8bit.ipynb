{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 preprocess_data.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 torchaudio==2.0.2+cu117 --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from torch.quantization import QuantStub, DeQuantStub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4953, 150, 9)\n",
      "X_test shape: (1320, 150, 9)\n",
      "y_train shape: (4953,)\n",
      "y_test shape: (1320,)\n",
      "Unique values in y_train: [ 0  1  3  5  7  8 11 15 21 23 25]\n",
      "Unique values in y_test: [ 0  1  3  5  7  8 11 15 21 23 25]\n",
      "Train data shape after reshape: (4953, 1, 150, 9)\n",
      "Test data shape after reshape: (1320, 1, 150, 9)\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets from the provided files\n",
    "X_train = np.load('./mesl_data/x_train.npy')\n",
    "X_test = np.load('./mesl_data/x_test.npy')\n",
    "y_train = np.load('./mesl_data/y_train.npy')\n",
    "y_test = np.load('./mesl_data/y_test.npy')\n",
    "\n",
    "\n",
    "# Verify the shapes of the data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Check the unique values in y_train and y_test\n",
    "print(\"Unique values in y_train:\", np.unique(y_train))\n",
    "print(\"Unique values in y_test:\", np.unique(y_test))\n",
    "\n",
    "# Ensure all labels are within the correct range\n",
    "assert np.all((y_train >= 0) & (y_train < 26)), \"y_train contains out-of-range values\"\n",
    "assert np.all((y_test >= 0) & (y_test < 26)), \"y_test contains out-of-range values\"\n",
    "\n",
    "# Normalize data\n",
    "def normalize(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    return (data - mean) / std\n",
    "\n",
    "# Normalize the data\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "# Convert data to float32\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)  # Ensuring labels are in int64\n",
    "y_test = y_test.astype(np.int64)    # Ensuring labels are in int64\n",
    "\n",
    "# Reshape data for the model\n",
    "X_train = X_train.reshape((-1, 1, X_train.shape[1], X_train.shape[2]))\n",
    "X_test = X_test.reshape((-1, 1, X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "print(f\"Train data shape after reshape: {X_train.shape}\")\n",
    "print(f\"Test data shape after reshape: {X_test.shape}\")\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded parameters for the new dataset\n",
    "NB_SENSOR_CHANNELS = 9\n",
    "NUM_CLASSES = 26  # Updated number of classes\n",
    "SLIDING_WINDOW_LENGTH = 150\n",
    "BATCH_SIZE = 16\n",
    "NUM_FILTERS = 64\n",
    "FILTER_SIZE = 5\n",
    "NUM_UNITS_LSTM = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class QuantizedLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(QuantizedLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "\n",
    "        self.ih = nn.Linear(input_size, 4 * hidden_size, bias=bias)\n",
    "        self.hh = nn.Linear(hidden_size, 4 * hidden_size, bias=bias)\n",
    "\n",
    "        self.quant1 = QuantStub()\n",
    "        self.dequant1 = DeQuantStub()\n",
    "\n",
    "        self.quant2 = QuantStub()\n",
    "        self.dequant2 = DeQuantStub()\n",
    "\n",
    "    def forward(self, input, hx):\n",
    "        hx, cx = hx\n",
    "\n",
    "        input = self.quant1(input)\n",
    "        hx = self.quant1(hx)\n",
    "        cx = self.quant2(cx)\n",
    "\n",
    "        gates = self.ih(input) + self.hh(hx)\n",
    "        i, f, g, o = gates.chunk(4, 1)\n",
    "\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        g = torch.tanh(g)\n",
    "        o = torch.sigmoid(o)\n",
    "\n",
    "        cy = f * cx + i * g\n",
    "        hy = o * torch.tanh(cy)\n",
    "\n",
    "        hy = self.dequant1(hy)\n",
    "        cy = self.dequant2(cy)\n",
    "\n",
    "        return hy, cy\n",
    "\n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(batch_size, self.hidden_size).zero_().to(device),\n",
    "                weight.new(batch_size, self.hidden_size).zero_().to(device))\n",
    "    \n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "'''\n",
    "class QATDeepConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QATDeepConvLSTM, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(1, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv2 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv3 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv4 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.lstm1 = nn.LSTM(NUM_FILTERS * NB_SENSOR_CHANNELS, NUM_UNITS_LSTM, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(NUM_UNITS_LSTM, NUM_UNITS_LSTM, batch_first=True)\n",
    "        self.fc = nn.Linear(NUM_UNITS_LSTM, NUM_CLASSES)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "\n",
    "        self.quant1 = torch.quantization.QuantStub()\n",
    "        self.quant2 = torch.quantization.QuantStub()\n",
    "        self.quant3 = torch.quantization.QuantStub()\n",
    "        self.quant4 = torch.quantization.QuantStub()\n",
    "        self.dequant1 = torch.quantization.DeQuantStub()\n",
    "        self.dequant2 = torch.quantization.DeQuantStub()\n",
    "        self.dequant3 = torch.quantization.DeQuantStub()\n",
    "        self.dequant4 = torch.quantization.DeQuantStub()\n",
    "\n",
    "        self.quant_lstm1 = torch.quantization.QuantStub()\n",
    "        self.quant_lstm2 = torch.quantization.QuantStub()\n",
    "        self.dequant_lstm1 = torch.quantization.DeQuantStub()\n",
    "        self.dequant_lstm2 = torch.quantization.DeQuantStub()\n",
    "\n",
    "        self.quant_fc = torch.quantization.QuantStub()\n",
    "        self.dequant_fc = torch.quantization.DeQuantStub()\n",
    "        # Weight initialization\n",
    "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv3.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv4.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.quant1(x)\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        #x = self.dequant1(x)\n",
    "        x = self.quant2(x)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.dequant2(x)\n",
    "        x = self.quant3(x)\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.dequant3(x)\n",
    "        x = self.quant4(x)\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.dequant4(x)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(x.size(0), x.size(2), -1)\n",
    "        \n",
    "        x = self.quant_lstm1(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dequant_lstm1(x)\n",
    "\n",
    "        x = self.quant_lstm2(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dequant_lstm2(x)\n",
    "\n",
    "        x = self.quant_fc(x)\n",
    "        x = self.fc(x[:, -1, :])\n",
    "        x = self.dequant_fc(x)\n",
    "\n",
    "        #x = self.dequant(x)\n",
    "        return x\n",
    "'''\n",
    "# Define the QAT-ready network\n",
    "class QATDeepConvLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QATDeepConvLSTM, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(1, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv2 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv3 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.conv4 = nn.Conv2d(NUM_FILTERS, NUM_FILTERS, (FILTER_SIZE, 1))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm1 = nn.LSTM(NUM_FILTERS * NB_SENSOR_CHANNELS, NUM_UNITS_LSTM, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(NUM_UNITS_LSTM, NUM_UNITS_LSTM, batch_first=True)\n",
    "        self.fc = nn.Linear(NUM_UNITS_LSTM, NUM_CLASSES)\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = x.permute(0, 2, 1, 3).contiguous().view(x.size(0), x.size(2), -1)\n",
    "        \n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        \n",
    "        x = self.fc(x[:, -1, :])\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized and prepared for QAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keerthiv/.local/lib/python3.9/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.quantization as quant\n",
    "from torch.quantization.observer import MovingAverageMinMaxObserver, default_weight_observer\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "\n",
    "class CustomObserver(MovingAverageMinMaxObserver):\n",
    "    def calculate_qparams(self):\n",
    "        scale, _ = super().calculate_qparams()\n",
    "        zero_point = torch.tensor(0, dtype=torch.int32)\n",
    "        return scale, zero_point\n",
    "# Create datasets and dataloaders\n",
    "train_data = torch.utils.data.TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "test_data = torch.utils.data.TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize the QAT-ready model, loss function, and optimizer\n",
    "model = QATDeepConvLSTM().to(device)\n",
    "\n",
    "'''\n",
    "model.qconfig = quant.QConfig(\n",
    "    activation=quant.FakeQuantize.with_args(observer=CustomObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine),\n",
    "    weight=quant.FakeQuantize.with_args(observer=default_weight_observer, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
    ")\n",
    "'''\n",
    "torch.backends.quantized.engine = 'x86'\n",
    "#model = torch.quantization.QuantWrapper(model)\n",
    "\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig('x86')\n",
    "\n",
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "print(\"Model initialized and prepared for QAT\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Calibrate the model with representative data\n",
    "calibration_size = int(0.1 * len(train_data))\n",
    "calibration_indices = np.random.choice(len(train_data), calibration_size, replace=False)\n",
    "calibration_subset = Subset(train_data, calibration_indices)\n",
    "calibration_loader = DataLoader(calibration_subset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keerthiv/.local/lib/python3.9/site-packages/torch/ao/quantization/fake_quantize.py:342: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:65.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/keerthiv/.local/lib/python3.9/site-packages/torch/ao/quantization/fake_quantize.py:342: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:651.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Loss: 2.6766617205835157\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "# Training loop with QAT\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {train_loss / len(train_loader)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QATDeepConvLSTM(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.1443], device='cuda:0'), zero_point=tensor([57], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.175763130187988, max_val=10.149259567260742)\n",
       "    )\n",
       "  )\n",
       "  (conv1): Conv2d(\n",
       "    1, 64, kernel_size=(5, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0032, 0.0017, 0.0026, 0.0029, 0.0034, 0.0034, 0.0031, 0.0034, 0.0030,\n",
       "              0.0034, 0.0031, 0.0032, 0.0035, 0.0031, 0.0027, 0.0027, 0.0024, 0.0017,\n",
       "              0.0032, 0.0035, 0.0028, 0.0017, 0.0031, 0.0011, 0.0033, 0.0021, 0.0032,\n",
       "              0.0031, 0.0032, 0.0032, 0.0030, 0.0033, 0.0030, 0.0032, 0.0029, 0.0024,\n",
       "              0.0026, 0.0029, 0.0027, 0.0031, 0.0029, 0.0031, 0.0033, 0.0032, 0.0030,\n",
       "              0.0017, 0.0035, 0.0011, 0.0030, 0.0033, 0.0033, 0.0034, 0.0028, 0.0031,\n",
       "              0.0020, 0.0029, 0.0034, 0.0032, 0.0034, 0.0029, 0.0016, 0.0034, 0.0022,\n",
       "              0.0033], device='cuda:0'), zero_point=tensor([   0,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "                 0,    0,    0,    0,    0, -128,    0,    0,    0,    0,    0, -128,\n",
       "                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "                 0,    0,    0,    0,    0,    0,    0,    0,    0,  127,    0,  127,\n",
       "                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              -128,    0,    0,    0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.2138, -0.4251, -0.1352, -0.3705, -0.2704, -0.2897, -0.3967, -0.3618,\n",
       "                -0.3857, -0.3169, -0.3664, -0.3421, -0.4005, -0.2756, -0.0606, -0.3195,\n",
       "                -0.3043,  0.0400, -0.3914, -0.2375, -0.1053, -0.1524, -0.4023,  0.0010,\n",
       "                -0.4204, -0.2718, -0.4037, -0.3932, -0.3372, -0.0776, -0.3609, -0.4174,\n",
       "                -0.1177, -0.4128, -0.2535, -0.3050, -0.3281, -0.3000, -0.3421, -0.3995,\n",
       "                -0.2054, -0.2495, -0.4046, -0.3122, -0.3885, -0.4385, -0.0500, -0.2741,\n",
       "                -0.1817, -0.4271, -0.3084, -0.2109, -0.1104, -0.3994, -0.1731, -0.3755,\n",
       "                -0.4410, -0.4147, -0.3244, -0.1471,  0.2345, -0.2957, -0.2811, -0.4218],\n",
       "               device='cuda:0'), max_val=tensor([ 0.4123, -0.2049,  0.3327,  0.0182,  0.4328,  0.4278,  0.2636,  0.4294,\n",
       "                 0.0484,  0.4333,  0.3913,  0.4081,  0.4470,  0.3895,  0.3418,  0.3402,\n",
       "                 0.2619,  0.4224,  0.4013,  0.4446,  0.3557,  0.2157,  0.0881,  0.2932,\n",
       "                 0.4069,  0.0447,  0.3961,  0.3131,  0.4031,  0.4010,  0.3768,  0.3932,\n",
       "                 0.3794,  0.2850,  0.3731,  0.2247,  0.1116,  0.3633,  0.1685,  0.1132,\n",
       "                 0.3676,  0.3990,  0.4192,  0.4013,  0.3248, -0.2437,  0.4391, -0.0658,\n",
       "                 0.3833,  0.3529,  0.4153,  0.4345,  0.3561,  0.3442,  0.2570,  0.2258,\n",
       "                 0.3388,  0.3790,  0.4255,  0.3743,  0.4019,  0.4323,  0.1225,  0.2680],\n",
       "               device='cuda:0')\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.1601], device='cuda:0'), zero_point=tensor([65], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.480406761169434, max_val=9.852174758911133)\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(\n",
       "    64, 64, kernel_size=(5, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0005, 0.0005, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005,\n",
       "              0.0005, 0.0005, 0.0004, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
       "              0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004,\n",
       "              0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
       "              0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005,\n",
       "              0.0004, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005, 0.0004,\n",
       "              0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005,\n",
       "              0.0005], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
       "             dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.0526, -0.0566, -0.0571, -0.0536, -0.0569, -0.0540, -0.0580, -0.0578,\n",
       "                -0.0562, -0.0546, -0.0508, -0.0574, -0.0572, -0.0559, -0.0581, -0.0515,\n",
       "                -0.0490, -0.0557, -0.0519, -0.0573, -0.0571, -0.0571, -0.0583, -0.0572,\n",
       "                -0.0562, -0.0509, -0.0569, -0.0532, -0.0562, -0.0494, -0.0499, -0.0571,\n",
       "                -0.0583, -0.0547, -0.0528, -0.0501, -0.0586, -0.0550, -0.0503, -0.0557,\n",
       "                -0.0557, -0.0541, -0.0555, -0.0580, -0.0572, -0.0569, -0.0527, -0.0552,\n",
       "                -0.0569, -0.0569, -0.0472, -0.0575, -0.0560, -0.0566, -0.0573, -0.0577,\n",
       "                -0.0552, -0.0571, -0.0508, -0.0562, -0.0580, -0.0547, -0.0547, -0.0576],\n",
       "               device='cuda:0'), max_val=tensor([0.0580, 0.0574, 0.0563, 0.0575, 0.0570, 0.0579, 0.0579, 0.0580, 0.0575,\n",
       "                0.0575, 0.0585, 0.0570, 0.0567, 0.0573, 0.0583, 0.0579, 0.0583, 0.0579,\n",
       "                0.0585, 0.0559, 0.0579, 0.0579, 0.0583, 0.0565, 0.0579, 0.0577, 0.0547,\n",
       "                0.0577, 0.0570, 0.0585, 0.0580, 0.0573, 0.0582, 0.0582, 0.0586, 0.0583,\n",
       "                0.0582, 0.0577, 0.0584, 0.0577, 0.0583, 0.0580, 0.0558, 0.0573, 0.0577,\n",
       "                0.0566, 0.0581, 0.0584, 0.0560, 0.0584, 0.0585, 0.0571, 0.0583, 0.0566,\n",
       "                0.0578, 0.0580, 0.0581, 0.0569, 0.0585, 0.0571, 0.0571, 0.0583, 0.0580,\n",
       "                0.0576], device='cuda:0')\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0989], device='cuda:0'), zero_point=tensor([36], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.5697853565216064, max_val=8.995420455932617)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Conv2d(\n",
       "    64, 64, kernel_size=(5, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005,\n",
       "              0.0004, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005,\n",
       "              0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
       "              0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0004, 0.0005, 0.0004,\n",
       "              0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005,\n",
       "              0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005, 0.0004,\n",
       "              0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0004, 0.0005, 0.0005, 0.0004,\n",
       "              0.0005], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
       "             dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.0577, -0.0583, -0.0554, -0.0582, -0.0574, -0.0578, -0.0569, -0.0555,\n",
       "                -0.0563, -0.0561, -0.0538, -0.0573, -0.0535, -0.0563, -0.0580, -0.0577,\n",
       "                -0.0563, -0.0588, -0.0513, -0.0554, -0.0570, -0.0521, -0.0542, -0.0541,\n",
       "                -0.0562, -0.0580, -0.0577, -0.0545, -0.0556, -0.0537, -0.0520, -0.0545,\n",
       "                -0.0555, -0.0569, -0.0562, -0.0563, -0.0538, -0.0578, -0.0573, -0.0550,\n",
       "                -0.0537, -0.0574, -0.0561, -0.0550, -0.0572, -0.0540, -0.0585, -0.0571,\n",
       "                -0.0570, -0.0559, -0.0558, -0.0563, -0.0580, -0.0571, -0.0579, -0.0559,\n",
       "                -0.0549, -0.0548, -0.0575, -0.0561, -0.0561, -0.0578, -0.0551, -0.0583],\n",
       "               device='cuda:0'), max_val=tensor([0.0574, 0.0576, 0.0578, 0.0579, 0.0562, 0.0580, 0.0567, 0.0584, 0.0578,\n",
       "                0.0567, 0.0580, 0.0583, 0.0587, 0.0565, 0.0577, 0.0562, 0.0556, 0.0586,\n",
       "                0.0584, 0.0581, 0.0568, 0.0580, 0.0585, 0.0573, 0.0584, 0.0568, 0.0586,\n",
       "                0.0584, 0.0583, 0.0588, 0.0586, 0.0564, 0.0576, 0.0542, 0.0578, 0.0557,\n",
       "                0.0584, 0.0579, 0.0584, 0.0580, 0.0581, 0.0570, 0.0584, 0.0581, 0.0585,\n",
       "                0.0579, 0.0584, 0.0550, 0.0563, 0.0546, 0.0576, 0.0552, 0.0578, 0.0562,\n",
       "                0.0585, 0.0574, 0.0582, 0.0586, 0.0571, 0.0543, 0.0583, 0.0578, 0.0559,\n",
       "                0.0578], device='cuda:0')\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.1266], device='cuda:0'), zero_point=tensor([34], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.313997745513916, max_val=11.76570987701416)\n",
       "    )\n",
       "  )\n",
       "  (conv4): Conv2d(\n",
       "    64, 64, kernel_size=(5, 1), stride=(1, 1)\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0005, 0.0005, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005,\n",
       "              0.0005, 0.0005, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0004, 0.0004,\n",
       "              0.0004, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
       "              0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005,\n",
       "              0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005,\n",
       "              0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0004, 0.0005, 0.0005,\n",
       "              0.0005, 0.0004, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0005,\n",
       "              0.0005], device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0',\n",
       "             dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.0541, -0.0579, -0.0547, -0.0546, -0.0574, -0.0570, -0.0544, -0.0576,\n",
       "                -0.0577, -0.0577, -0.0529, -0.0533, -0.0557, -0.0569, -0.0565, -0.0541,\n",
       "                -0.0568, -0.0555, -0.0571, -0.0559, -0.0584, -0.0572, -0.0535, -0.0533,\n",
       "                -0.0540, -0.0550, -0.0580, -0.0571, -0.0577, -0.0555, -0.0560, -0.0536,\n",
       "                -0.0556, -0.0580, -0.0567, -0.0526, -0.0566, -0.0575, -0.0578, -0.0579,\n",
       "                -0.0537, -0.0552, -0.0586, -0.0555, -0.0582, -0.0566, -0.0556, -0.0543,\n",
       "                -0.0532, -0.0579, -0.0557, -0.0556, -0.0564, -0.0565, -0.0511, -0.0566,\n",
       "                -0.0559, -0.0582, -0.0539, -0.0536, -0.0529, -0.0556, -0.0555, -0.0556],\n",
       "               device='cuda:0'), max_val=tensor([0.0582, 0.0585, 0.0568, 0.0578, 0.0571, 0.0577, 0.0581, 0.0575, 0.0582,\n",
       "                0.0579, 0.0589, 0.0581, 0.0549, 0.0586, 0.0573, 0.0587, 0.0549, 0.0558,\n",
       "                0.0545, 0.0549, 0.0585, 0.0545, 0.0584, 0.0580, 0.0585, 0.0586, 0.0539,\n",
       "                0.0551, 0.0572, 0.0585, 0.0575, 0.0582, 0.0574, 0.0572, 0.0557, 0.0586,\n",
       "                0.0552, 0.0572, 0.0576, 0.0578, 0.0582, 0.0581, 0.0588, 0.0585, 0.0576,\n",
       "                0.0554, 0.0582, 0.0579, 0.0577, 0.0575, 0.0561, 0.0551, 0.0586, 0.0582,\n",
       "                0.0588, 0.0548, 0.0558, 0.0572, 0.0588, 0.0584, 0.0582, 0.0553, 0.0582,\n",
       "                0.0583], device='cuda:0')\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.1533], device='cuda:0'), zero_point=tensor([35], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.379786014556885, max_val=14.086615562438965)\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (lstm1): QuantizableLSTM(\n",
       "    (layers): ModuleList(\n",
       "      (0): _LSTMLayer(\n",
       "        (layer_fw): _LSTMSingleLayer(\n",
       "          (cell): QuantizableLSTMCell(\n",
       "            (igates): Linear(\n",
       "              in_features=576, out_features=512, bias=True\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.1958], device='cuda:0'), zero_point=tensor([52], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.087396621704102, max_val=14.773557662963867)\n",
       "              )\n",
       "            )\n",
       "            (hgates): Linear(\n",
       "              in_features=128, out_features=512, bias=True\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0324], device='cuda:0'), zero_point=tensor([54], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7391254901885986, max_val=2.3702499866485596)\n",
       "              )\n",
       "            )\n",
       "            (gates): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.1885], device='cuda:0'), zero_point=tensor([52], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.807735443115234, max_val=14.131433486938477)\n",
       "              )\n",
       "            )\n",
       "            (input_gate): Sigmoid()\n",
       "            (forget_gate): Sigmoid()\n",
       "            (cell_gate): Tanh()\n",
       "            (output_gate): Sigmoid()\n",
       "            (fgate_cx): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.1758], device='cuda:0'), zero_point=tensor([63], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.080257415771484, max_val=11.247427940368652)\n",
       "              )\n",
       "            )\n",
       "            (igate_cgate): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0157], device='cuda:0'), zero_point=tensor([64], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9999338388442993, max_val=0.9993957281112671)\n",
       "              )\n",
       "            )\n",
       "            (fgate_cx_igate_cgate): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.1838], device='cuda:0'), zero_point=tensor([63], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.599038124084473, max_val=11.743083953857422)\n",
       "              )\n",
       "            )\n",
       "            (ogate_cy): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0157], device='cuda:0'), zero_point=tensor([64], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9991054534912109, max_val=0.9987797737121582)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lstm2): QuantizableLSTM(\n",
       "    (layers): ModuleList(\n",
       "      (0): _LSTMLayer(\n",
       "        (layer_fw): _LSTMSingleLayer(\n",
       "          (cell): QuantizableLSTMCell(\n",
       "            (igates): Linear(\n",
       "              in_features=128, out_features=512, bias=True\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0328], device='cuda:0'), zero_point=tensor([57], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8810666799545288, max_val=2.279961347579956)\n",
       "              )\n",
       "            )\n",
       "            (hgates): Linear(\n",
       "              in_features=128, out_features=512, bias=True\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0227], device='cuda:0'), zero_point=tensor([50], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1442288160324097, max_val=1.7361265420913696)\n",
       "              )\n",
       "            )\n",
       "            (gates): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0432], device='cuda:0'), zero_point=tensor([50], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1696226596832275, max_val=3.3204987049102783)\n",
       "              )\n",
       "            )\n",
       "            (input_gate): Sigmoid()\n",
       "            (forget_gate): Sigmoid()\n",
       "            (cell_gate): Tanh()\n",
       "            (output_gate): Sigmoid()\n",
       "            (fgate_cx): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0736], device='cuda:0'), zero_point=tensor([66], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.846148490905762, max_val=4.496855735778809)\n",
       "              )\n",
       "            )\n",
       "            (igate_cgate): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0129], device='cuda:0'), zero_point=tensor([62], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8002117276191711, max_val=0.8409481644630432)\n",
       "              )\n",
       "            )\n",
       "            (fgate_cx_igate_cgate): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0825], device='cuda:0'), zero_point=tensor([64], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.313747406005859, max_val=5.1607208251953125)\n",
       "              )\n",
       "            )\n",
       "            (ogate_cy): FloatFunctional(\n",
       "              (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "                fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0149], device='cuda:0'), zero_point=tensor([64], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "                (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9534040689468384, max_val=0.9345620274543762)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(\n",
       "    in_features=128, out_features=26, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
       "              0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007,\n",
       "              0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007, 0.0007],\n",
       "             device='cuda:0'), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.0878, -0.0894, -0.0880, -0.0907, -0.0912, -0.0903, -0.0905, -0.0885,\n",
       "                -0.0899, -0.0844, -0.0905, -0.0906, -0.0915, -0.0907, -0.0908, -0.0895,\n",
       "                -0.0913, -0.0902, -0.0912, -0.0904, -0.0901, -0.0898, -0.0911, -0.0892,\n",
       "                -0.0906, -0.0897], device='cuda:0'), max_val=tensor([0.0893, 0.0883, 0.0885, 0.0905, 0.0902, 0.0880, 0.0894, 0.0879, 0.0895,\n",
       "                0.0907, 0.0899, 0.0849, 0.0910, 0.0893, 0.0899, 0.0892, 0.0906, 0.0890,\n",
       "                0.0904, 0.0896, 0.0895, 0.0895, 0.0901, 0.0885, 0.0861, 0.0897],\n",
       "               device='cuda:0')\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([0.0160], device='cuda:0'), zero_point=tensor([66], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0553057193756104, max_val=0.9708146452903748)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to quantized version\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert the model to a quantized version\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "model_int8 = torch.quantization.convert(model, inplace=False)\n",
    "print(\"Model converted to quantized version\")\n",
    "\n",
    "# Save the quantized model parameters\n",
    "torch.save(model_int8.state_dict(), 'weights/QATDeepConvLSTM_trained_mesl_data.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QATDeepConvLSTM' object has no attribute 'bn1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conv\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Fold BatchNorm layers\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m fold_batch_norm(model\u001b[38;5;241m.\u001b[39mconv1, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m)\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mconv2 \u001b[38;5;241m=\u001b[39m fold_batch_norm(model\u001b[38;5;241m.\u001b[39mconv2, model\u001b[38;5;241m.\u001b[39mbn2)\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mconv3 \u001b[38;5;241m=\u001b[39m fold_batch_norm(model\u001b[38;5;241m.\u001b[39mconv3, model\u001b[38;5;241m.\u001b[39mbn2)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QATDeepConvLSTM' object has no attribute 'bn1'"
     ]
    }
   ],
   "source": [
    "def fold_batch_norm(conv, bn):\n",
    "    # Fold BatchNorm parameters into Convolution layer\n",
    "    with torch.no_grad():\n",
    "        scale_factor = bn.weight / torch.sqrt(bn.running_var + bn.eps)\n",
    "        conv.weight.copy_(conv.weight * scale_factor.reshape([-1, 1, 1, 1]))\n",
    "        if conv.bias is None:\n",
    "            conv.bias = torch.nn.Parameter(torch.zeros(conv.weight.size(0), dtype=conv.weight.dtype, device=conv.weight.device))\n",
    "        conv.bias.copy_((conv.bias - bn.running_mean) * scale_factor + bn.bias)\n",
    "    return conv\n",
    "\n",
    "# Fold BatchNorm layers\n",
    "model.conv1 = fold_batch_norm(model.conv1, model.bn1)\n",
    "model.conv2 = fold_batch_norm(model.conv2, model.bn2)\n",
    "model.conv3 = fold_batch_norm(model.conv3, model.bn2)\n",
    "model.conv4 = fold_batch_norm(model.conv4, model.bn2)\n",
    "\n",
    "# Remove BatchNorm layers\n",
    "model.bn1 = None\n",
    "model.bn2 = None\n",
    "model.bn3 = None\n",
    "model.bn4 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6220\n",
      "Macro Precision: 0.6498\n",
      "Macro Recall: 0.6220\n",
      "Macro F1-score: 0.6030\n",
      "Total inference time on CPU: 2.1234 seconds\n",
      "Average inference time per batch: 0.025475 seconds\n",
      "Average inference time per sample: 0.001609 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keerthiv/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "torch.backends.quantized.engine = 'x86'\n",
    "# Ensure the model is in evaluation mode\n",
    "model_int8.eval()\n",
    "model_int8.to('cpu')\n",
    "test_pred = []\n",
    "test_true = []\n",
    "\n",
    "batch_times = []\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Move inputs and targets to CPU and quantize the inputs\n",
    "        inputs = inputs.to('cpu')\n",
    "        #inputs_quantized = torch.quantize_per_tensor(inputs, scale=0.1, zero_point=0, dtype=torch.qint8)\n",
    "        #inputs_quantized.to('cpu')\n",
    "        targets = targets.to('cpu')\n",
    "        \n",
    "        # Perform inference\n",
    "        outputs = model_int8(inputs)\n",
    "        \n",
    "        # Dequantize the outputs for further processing\n",
    "        outputs_dequantized = outputs.dequantize()\n",
    "        \n",
    "        _, preds = torch.max(outputs_dequantized, 1)\n",
    "        batch_end_time = time.time()\n",
    "        \n",
    "        batch_times.append(batch_end_time - batch_start_time)\n",
    "        \n",
    "        test_pred.extend(preds.numpy())\n",
    "        test_true.extend(targets.numpy())\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_true, test_pred)\n",
    "macro_precision = precision_score(test_true, test_pred, average='macro')\n",
    "macro_recall = recall_score(test_true, test_pred, average='macro')\n",
    "macro_f1 = f1_score(test_true, test_pred, average='macro')\n",
    "\n",
    "# Results presentation\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.4f}\")\n",
    "print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "\n",
    "# Performance time\n",
    "total_inference_time = end_time - start_time\n",
    "average_batch_time = sum(batch_times) / len(batch_times)\n",
    "average_sample_time = total_inference_time / len(test_loader.dataset)\n",
    "\n",
    "print(f\"Total inference time on CPU: {total_inference_time:.4f} seconds\")\n",
    "print(f\"Average inference time per batch: {average_batch_time:.6f} seconds\")\n",
    "print(f\"Average inference time per sample: {average_sample_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set the quantized engine to 'qnnpack'\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "# Verify supported quantization engines\n",
    "print(f\"Supported quantized engines: {torch.backends.quantized.supported_engines}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
